{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "import random\n",
    "\n",
    "#I create a personalized corpus with two category: medical and other.\n",
    "#500 files are medical and 500 files are other.\n",
    "my_corpus = CategorizedPlaintextCorpusReader('corpora', r\"(?!\\.).*\\.txt\", cat_pattern=r\"(medical|other)/.*\")\n",
    "\n",
    "#I create a list of file where every file is categorized in medical or other.\n",
    "#In my_documents[i][0] for i in range(len(my_documents)) I have access to the text of the file.\n",
    "#In my documents[i][1] for i in range(len(my_documents)) I have access to the category of the file.\n",
    "my_documents = [(list(my_corpus.words(fileid)), category) for category in my_corpus.categories() for fileid in my_corpus.fileids(category)]\n",
    "random.shuffle(my_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#This function process a document take from my_documents.\n",
    "def processed_document(document):\n",
    "    text = document\n",
    "    #In this point I eliminate from the document every word that isn't an alfanumeric word (ex.: '(', ',', '.', '[' etc.).\n",
    "    text_only_alnum = [word for word in text if word.isalnum()]\n",
    "    #In this point I eliminate from the document every number.\n",
    "    text_without_digit = [word for word in text_only_alnum if not word.isdigit()]\n",
    "    #In this point I eliminate from the document every english stop words.\n",
    "    sw_remove_text = [word for word in text_without_digit if not word in stop_words]\n",
    "    #In the final two point I use the stemming and lemmatization method.\n",
    "    stemmed = [PorterStemmer().stem(w) for w in sw_remove_text]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in stemmed]  \n",
    "    document.clear()\n",
    "    for word in lemmed:\n",
    "        document.append(word)\n",
    "        \n",
    "for i in range(len(my_documents)):\n",
    "    processed_document(my_documents[i][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#I select the first 2000 most frequent words from my personalized corpus.\n",
    "all_words = nltk.FreqDist(w for w in my_corpus.words())\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "#This function is used as a feature extractor.\n",
    "#The feature extractor simply checks whether each of these words is present in a given document.\n",
    "def document_features(document, words): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in my_documents]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f23f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a70ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#This function is used to process a wikipedia page.\n",
    "#Is very similar to the function 'processed_document(document)'.\n",
    "def processed_wiki_text(wiki_text):\n",
    "    #In this point I tokenize the page with a word tokenizer.\n",
    "    tokenize_text = word_tokenize(wiki_text)\n",
    "    #The other points are equals to the points of the function 'processed_document(document)' \n",
    "    text_only_alnum = [word for word in tokenize_text if word.isalnum()]\n",
    "    text_without_digit = [word for word in text_only_alnum if not word.isdigit()]\n",
    "    sw_remove_text = [word for word in text_without_digit if not word in stop_words]\n",
    "    stemmed = [PorterStemmer().stem(w) for w in sw_remove_text]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in stemmed]\n",
    "    \n",
    "    return lemmed\n",
    "\n",
    "print(\"If you want to terminate the program leave the url input in blank!\")\n",
    "\n",
    "while True:\n",
    "    url = input(\"Insert the URL of wikipedia topic: \\n\")\n",
    "    \n",
    "    if url == \"\":\n",
    "        break\n",
    "\n",
    "    # get URL\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # scrape webpage\n",
    "    soup = BeautifulSoup(page.content.lower(), 'html.parser')\n",
    "\n",
    "    my_wiki = \"\"\n",
    "\n",
    "    for i in range(len(soup.find_all('p'))):\n",
    "        my_wiki += soup.find_all('p')[i].get_text()\n",
    "\n",
    "    processed_wiki = processed_wiki_text(my_wiki)\n",
    "    all_wiki_words = nltk.FreqDist(w for w in processed_wiki)\n",
    "    wiki_word_features = list(all_wiki_words)[:20]\n",
    "\n",
    "    wiki_features = document_features(processed_wiki, wiki_word_features)\n",
    "\n",
    "    category = classifier.classify(wiki_features)\n",
    "    print(\"Category: \", category, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
